{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GO6tSi7bmXuT"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Szupt8WGmkaT"
      },
      "outputs": [],
      "source": [
        "# carregando o url e o header\n",
        "url = 'https://keithgalli.github.io/web-scraping/'\n",
        "header = {'User-Agent':'Mozilla/5.0'}\n",
        "\n",
        "# criando a sopa\n",
        "r = requests.get(url + 'webpage.html', headers=header)\n",
        "soup = bs(r.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrJLZQgaUfhZ"
      },
      "outputs": [],
      "source": [
        "# busque todos os links de rede social da página\n",
        "\n",
        "social_links = soup.find_all('li', class_='social')\n",
        "for i in social_links:\n",
        "  b = i.find('b')\n",
        "  a = i.find('a')['href']\n",
        "  print(b.string,a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWX4XWBjYOMP"
      },
      "outputs": [],
      "source": [
        "# trasporte a tabela\n",
        "\n",
        "# importando Pandas para auxiliar na importação\n",
        "import pandas as pd\n",
        "\n",
        "# retirando a tabela\n",
        "table = soup.find('table', class_='hockey-stats')\n",
        "\n",
        "# retirando a header\n",
        "columns = table.find_all('th')\n",
        "column_names = [c.string for c in columns]\n",
        "\n",
        "# retirando as linhas\n",
        "rows = table.find('tbody').find_all('tr')\n",
        "\n",
        "# inserindo no DataFrame\n",
        "l = []\n",
        "for tr in rows:\n",
        "  td = tr.find_all('td')\n",
        "  row = [str(tr.get_text()).strip() for tr in td]\n",
        "  l.append(row)\n",
        "  df = pd.DataFrame(l,columns=column_names)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjrwgFlTc8Oo"
      },
      "outputs": [],
      "source": [
        "# exportando todos os fun facts que utilizam a palavra 'is'\n",
        "\n",
        "facts = soup.find('ul', class_='fun-facts').find_all('li')\n",
        "fact_texts = [f.get_text() for f in facts]\n",
        "\n",
        "# imporando Regex para ajudar na pesquisa\n",
        "import re\n",
        "\n",
        "# checagem da palavra\n",
        "rex = re.compile(rf'\\bis\\b',re.IGNORECASE)\n",
        "with_is = [fact for fact in fact_texts if rex.search(fact)]\n",
        "with_is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nXBU7kuBqoQ9"
      },
      "outputs": [],
      "source": [
        "# download das imagens da página\n",
        "\n",
        "# exportando o source das imagens\n",
        "img = soup.select('img')\n",
        "img_src = [i['src'] for i in img]\n",
        "img_src = img_src[0:4]\n",
        "\n",
        "# acrescentando o url ao url do site\n",
        "img_url = [url+i for i in img_src]\n",
        "\n",
        "# download de imagem\n",
        "for i in img_url:\n",
        "  img_data = requests.get(i).content\n",
        "  with open('photo.jpg', 'wb') as handler:\n",
        "      handler.write(img_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resolvendo o Desafio Misterioso\n",
        "\n",
        "# selecionando os arquivos com a mensagem misteriosa\n",
        "files = soup.select('div ul li a')\n",
        "files_href = [f['href'] for f in files]\n",
        "files_src = [url+f for f in files_href]\n",
        "\n",
        "# scraping os arquivos juntamente com as mensagens misteriosas\n",
        "phrase = []\n",
        "for f in files_src:\n",
        "  r = requests.get(f, headers=header)\n",
        "  s = bs(r.content)\n",
        "  p = s.find('p',attrs={'id':'secret-word'})\n",
        "  p_text = p.text\n",
        "  phrase.append(p_text)\n",
        "\n",
        "# imprimindo a mensagem\n",
        "for p in phrase:\n",
        "  print(p)"
      ],
      "metadata": {
        "id": "v_cbZF4GtMQ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}